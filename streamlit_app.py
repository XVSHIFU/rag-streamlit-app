import streamlit as st
import os
import sys
from dotenv import load_dotenv

# LangChain æ ¸å¿ƒ
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableBranch, RunnablePassthrough

# å‘é‡æ•°æ®åº“å’Œ Embeddings
from langchain_community.vectorstores import Chroma
from langchain.embeddings.base import Embeddings

# æ™ºè°±å¤§æ¨¡å‹
from zhipuai_llm import ZhipuaiLLM

# åŠ è½½ .env (æœ¬åœ°è°ƒè¯•ç”¨)
load_dotenv()


# ----------- è‡ªå®šä¹‰æ™ºè°± Embeddings ç±» -----------
class ZhipuAIEmbeddings(Embeddings):
    def __init__(self):
        from zhipuai import ZhipuAI
        api_key = os.environ.get("ZHIPUAI_API_KEY")
        if not api_key:
            raise ValueError("æœªæä¾› ZHIPUAI_API_KEYï¼Œè¯·åœ¨ Streamlit Secrets ä¸­é…ç½®")
        self.client = ZhipuAI(api_key=api_key)

    def embed_documents(self, texts):
        return [self.client.embedding(text)["embedding"] for text in texts]

    def embed_query(self, text):
        return self.client.embedding(text)["embedding"]


# ----------- è·å–å‘é‡æ£€ç´¢å™¨ -----------
def get_retriever(documents=None):
    embedding = ZhipuAIEmbeddings()

    # å¦‚æœæ²¡æœ‰ä¼ æ–‡æ¡£ï¼Œä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£
    if documents is None:
        documents = [
            {"page_content": "å—ç“œä¹¦æ˜¯ã€Šæœºå™¨å­¦ä¹ ã€‹ï¼ˆè¥¿ç“œä¹¦ï¼‰çš„é…å¥—è¾…å¯¼ä¹¦ï¼Œç”¨äºå¸®åŠ©ç†è§£è¥¿ç“œä¹¦çš„å†…å®¹ã€‚"},
            {"page_content": "Prompt Engineering æ˜¯ä¸ºå¤§è¯­è¨€æ¨¡å‹è®¾è®¡å’Œä¼˜åŒ–æç¤ºçš„æŠ€æœ¯ã€‚"}
        ]

    vectordb = Chroma.from_documents(
        documents,
        embedding_function=embedding,
        persist_directory=None  # Cloud ä¸Šä¸å†™å…¥ç£ç›˜
    )
    return vectordb.as_retriever(search_kwargs={"k": 3})


# ----------- æ–‡æ¡£åˆå¹¶å‡½æ•° -----------
def combine_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs["context"])


# ----------- æ„å»º QA é“¾ -----------
def get_qa_history_chain():
    retriever = get_retriever()
    llm = ZhipuaiLLM(model_name="glm-4-plus", temperature=0.1, api_key=os.environ.get("ZHIPUAI_API_KEY"))

    condense_question_prompt = ChatPromptTemplate([
        ("system", "è¯·æ ¹æ®èŠå¤©è®°å½•æ€»ç»“ç”¨æˆ·æœ€è¿‘çš„é—®é¢˜ï¼Œå¦‚æœæ²¡æœ‰å¤šä½™èŠå¤©è®°å½•åˆ™è¿”å›ç”¨æˆ·çš„é—®é¢˜ã€‚"),
        ("placeholder", "{chat_history}"),
        ("human", "{input}")
    ])

    retrieve_docs = RunnableBranch(
        (lambda x: not x.get("chat_history", False), (lambda x: x["input"]) | retriever),
        condense_question_prompt | llm | StrOutputParser() | retriever,
    )

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯é—®ç­”åŠ©æ‰‹ã€‚ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸çŸ¥é“å°±è¯´ä¸çŸ¥é“ï¼Œç®€æ´å›ç­”ã€‚\n\n{context}"),
        ("placeholder", "{chat_history}"),
        ("human", "{input}")
    ])

    qa_chain = (
        RunnablePassthrough().assign(context=combine_docs)
        | qa_prompt
        | llm
        | StrOutputParser()
    )

    qa_history_chain = RunnablePassthrough().assign(
        context=retrieve_docs
    ).assign(answer=qa_chain)

    return qa_history_chain


# ----------- æµå¼å›ç­”ç”Ÿæˆ -----------
def gen_response(chain, input, chat_history):
    response = chain.stream({
        "input": input,
        "chat_history": chat_history
    })
    for res in response:
        if "answer" in res:
            yield res["answer"]


# ----------- Streamlit ç•Œé¢ -----------
def main():
    st.markdown("### ğŸ¦œğŸ”— RAG å¤§æ¨¡å‹é—®ç­”æ¼”ç¤º")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    if "qa_history_chain" not in st.session_state:
        st.session_state.qa_history_chain = get_qa_history_chain()

    messages_container = st.container(height=550)

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for role, msg in st.session_state.messages:
        with messages_container.chat_message(role):
            st.write(msg)

    if prompt := st.chat_input("è¯·è¾“å…¥é—®é¢˜"):
        st.session_state.messages.append(("human", prompt))
        with messages_container.chat_message("human"):
            st.write(prompt)

        answer_stream = gen_response(
            chain=st.session_state.qa_history_chain,
            input=prompt,
            chat_history=st.session_state.messages
        )

        with messages_container.chat_message("ai"):
            output = st.write_stream(answer_stream)

        st.session_state.messages.append(("ai", output))


if __name__ == "__main__":
    main()
